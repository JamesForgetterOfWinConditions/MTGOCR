{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxxGnftVhBwdKzjzjugmFH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesForgetterOfWinConditions/MTGOCR/blob/main/SorceryProxies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY9S0YdlUyND",
        "outputId": "f7e861bd-cb3b-47ee-e210-61d48e709111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 651 card image URLs and saved them to card_image_urls.txt\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import urllib.parse\n",
        "\n",
        "# Load HAR file which is manual network download of fully loaded curiosa.io/cards page\n",
        "with open(\"curiosa.io1.har\", \"r\", encoding=\"utf-8\") as f:\n",
        "    har_data = json.load(f)\n",
        "\n",
        "urls = []\n",
        "for entry in har_data.get(\"log\", {}).get(\"entries\", []):\n",
        "    url = entry.get(\"request\", {}).get(\"url\", \"\")\n",
        "    if \"curiosa.io/_next/image?url=\" in url:\n",
        "        # Extract and decode the original CloudFront image URL\n",
        "        decoded = urllib.parse.unquote(url.split(\"url=\")[1].split(\"&\")[0])\n",
        "        if decoded not in urls:\n",
        "            urls.append(decoded)\n",
        "\n",
        "# Save to file\n",
        "with open(\"card_image_urls.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(urls))\n",
        "\n",
        "print(f\"Found {len(urls)} card image URLs and saved them to card_image_urls.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "339d13c0",
        "outputId": "e6dd6527-220d-4cc2-8545-900f09f158f5"
      },
      "source": [
        "# Creates database of all cards\n",
        "import os\n",
        "import re\n",
        "import urllib.parse\n",
        "\n",
        "# Path to your file\n",
        "file_path = \"card_image_urls.txt\"\n",
        "\n",
        "# Output dictionary\n",
        "cards = {}\n",
        "\n",
        "# Read all URLs\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        url = line.strip()\n",
        "        if not url:\n",
        "            continue\n",
        "\n",
        "        # Extract filename part\n",
        "        parsed_url = urllib.parse.urlparse(url)\n",
        "        filename = os.path.basename(parsed_url.path)  # e.g. avatar_of_air_d_s.png\n",
        "\n",
        "        # Clean the name (remove suffixes and underscores)\n",
        "        name = re.sub(r\"[_-](b|d|s)?(_[a-z])?\\.png$\", \"\", filename, flags=re.IGNORECASE)\n",
        "        name = name.replace(\"_\", \" \").title()\n",
        "\n",
        "        cards[name] = url\n",
        "\n",
        "# Save to a CSV for convenience\n",
        "with open(\"card_name_url_pairs.csv\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"name,url\\n\")\n",
        "    for name, url in cards.items():\n",
        "        f.write(f'\"{name}\",\"{url}\"\\n')\n",
        "\n",
        "print(f\"Extracted {len(cards)} cards → saved to card_name_url_pairs.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 651 cards → saved to card_name_url_pairs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70327c8d",
        "outputId": "e6959c35-b6df-4bb8-c9c1-b327ca43df42"
      },
      "source": [
        "# Create proxyPDF from database and decklist.txt\n",
        "# Decklist.txt needs to have special characters removed to match the database (ex ! and ')\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from fpdf import FPDF\n",
        "\n",
        "# === CONFIG ===\n",
        "CSV_PATH = \"card_name_url_pairs.csv\"  # your CSV of name→URL\n",
        "DECK_PATH = \"decklist.txt\"            # your formatted decklist\n",
        "OUTPUT_DIR = \"proxy_images\"\n",
        "PDF_PATH = \"proxies.pdf\"\n",
        "\n",
        "# === STEP 1: Load card name → URL mapping ===\n",
        "card_map = {}\n",
        "with open(CSV_PATH, encoding=\"utf-8\") as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        card_map[row[\"name\"].strip().lower()] = row[\"url\"].strip()\n",
        "\n",
        "# === STEP 2: Read decklist ===\n",
        "deck = []\n",
        "with open(DECK_PATH, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith(\"#\"):\n",
        "            continue\n",
        "        match = re.match(r\"(\\d+)\\s+(.+)\", line)\n",
        "        if match:\n",
        "            count = int(match.group(1))\n",
        "            name = match.group(2).strip().lower()\n",
        "            deck.append((name, count))\n",
        "        else:\n",
        "            deck.append((line.lower(), 1))\n",
        "\n",
        "print(f\"Loaded {len(deck)} deck entries from {DECK_PATH}\")\n",
        "\n",
        "# === STEP 3: Download and prepare images ===\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "images = []\n",
        "\n",
        "for name, count in deck:\n",
        "    url = None\n",
        "\n",
        "    # Find best match (exact or partial)\n",
        "    if name in card_map:\n",
        "        url = card_map[name]\n",
        "    else:\n",
        "        for k in card_map:\n",
        "            if name in k:\n",
        "                url = card_map[k]\n",
        "                break\n",
        "\n",
        "    if not url:\n",
        "        print(f\"⚠️  Card not found: {name}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Downloading {name} x{count}\")\n",
        "    try:\n",
        "        img_data = requests.get(url, timeout=20).content\n",
        "        img = Image.open(BytesIO(img_data)).convert(\"RGB\")\n",
        "        for _ in range(count):\n",
        "            images.append(img)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {name}: {e}\")\n",
        "\n",
        "# === STEP 4: Generate proxy PDF (Letter size) ===\n",
        "if not images:\n",
        "    raise SystemExit(\"No card images found! Check decklist or CSV mapping.\")\n",
        "\n",
        "pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"Letter\")\n",
        "pdf.add_page()\n",
        "\n",
        "# Card layout: 3 columns × 3 rows (≈63×88 mm per card)\n",
        "card_width, card_height = 63, 88\n",
        "cols, rows = 3, 3\n",
        "x, y = 10, 10\n",
        "margin_x, margin_y = 10, 10\n",
        "\n",
        "for i, img in enumerate(images):\n",
        "    tmp_path = os.path.join(OUTPUT_DIR, f\"tmp_{i}.jpg\")\n",
        "    img_resized = img.resize(\n",
        "        (int(card_width / 25.4 * 300), int(card_height / 25.4 * 300))\n",
        "    )\n",
        "    img_resized.save(tmp_path, \"JPEG\")\n",
        "    pdf.image(tmp_path, x, y, card_width, card_height)\n",
        "\n",
        "    # Position next card\n",
        "    x += card_width + .2\n",
        "    if (i + 1) % cols == 0:\n",
        "        x = margin_x\n",
        "        y += card_height + .2\n",
        "    if (i + 1) % (cols * rows) == 0 and i + 1 < len(images):\n",
        "        pdf.add_page()\n",
        "        x, y = margin_x, margin_y\n",
        "\n",
        "pdf.output(PDF_PATH)\n",
        "print(f\"✅ Created printable proxy sheet: {PDF_PATH}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 46 deck entries from decklist.txt\n",
            "Downloading avatar x1\n",
            "Downloading archimago x1\n",
            "Downloading the black plague x1\n",
            "Downloading the great famine x1\n",
            "Downloading ring of morrigan x1\n",
            "Downloading angels egg x2\n",
            "Downloading chains of prometheus x2\n",
            "Downloading kettletop leprechaun x4\n",
            "Downloading root spider x3\n",
            "Downloading grandmaster wizard x2\n",
            "Downloading browse x1\n",
            "Downloading divine healing x3\n",
            "Downloading pendragon legacy x1\n",
            "Downloading blink x4\n",
            "Downloading common sense x4\n",
            "Downloading dispel x1\n",
            "Downloading lightning bolt x2\n",
            "Downloading magic missiles x2\n",
            "Downloading power of flight x1\n",
            "Downloading border militia x1\n",
            "Downloading bury x1\n",
            "Downloading feast for crows x1\n",
            "Downloading chaos twister x2\n",
            "Downloading gigantism x1\n",
            "Downloading earthquake x2\n",
            "Downloading poison nova x3\n",
            "Downloading guards x1\n",
            "Downloading major explosion x1\n",
            "Downloading craterize x2\n",
            "Downloading annual fair x2\n",
            "Downloading dark tower x1\n",
            "Downloading fields of camlann x1\n",
            "Downloading funeral pyre x2\n",
            "Downloading glastonbury tor x1\n",
            "Downloading gnome hollows x3\n",
            "Downloading gothic tower x1\n",
            "Downloading holy ground x1\n",
            "Downloading lone tower x1\n",
            "Downloading lookout x1\n",
            "Downloading mirror realm x1\n",
            "Downloading pebbled paths x3\n",
            "Downloading perilous bridge x2\n",
            "Downloading pillar of zeiros x1\n",
            "Downloading ruins x3\n",
            "Downloading steppe x3\n",
            "Downloading windmill x3\n",
            "✅ Created printable proxy sheet: proxies.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pillow fpdf requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysUeco-pbR_e",
        "outputId": "288014a2-4b06-4449-c4bd-4a43efff0b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=ef6d88f3fb828af28ae21dc8a92f4a5fd55128b619f699b463443d32417b60df\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/62/11/dc73d78e40a218ad52e7451f30166e94491be013a7850b5d75\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen posixpath>:82: RuntimeWarning: coroutine 'main' was never awaited\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    }
  ]
}